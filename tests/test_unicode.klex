// Test Unicode character ranges
%%
[\u{1F600}-\u{1F64F}]+ -> EMOJI_EMOTICONS
[\u{1F300}-\u{1F5FF}]+ -> EMOJI_MISC_SYMBOLS
[\u{1F680}-\u{1F6FF}]+ -> EMOJI_TRANSPORT
[\u{3040}-\u{309F}]+ -> HIRAGANA
[\u{30A0}-\u{30FF}]+ -> KATAKANA
[\u{4E00}-\u{9FFF}]+ -> KANJI
[\u{AC00}-\u{D7AF}]+ -> HANGUL
[\u{0400}-\u{04FF}]+ -> CYRILLIC
[\u{0600}-\u{06FF}]+ -> ARABIC
[\x41-\x5A]+ -> UPPERCASE_HEX
[\x61-\x7A]+ -> LOWERCASE_HEX
[0-9]+ -> NUMBER
[ \t\n\r]+ -> _
%%

#[cfg(test)]
mod tests {
    use super::*;

    fn TestUnicodeLexer(str: &str) -> Lexer {
        Lexer::from_str(str)
    }

    #[test]
    fn test_emoji_emoticons() {
        let input = "ğŸ˜€ğŸ˜ğŸ˜‚";
        let mut lexer = TestUnicodeLexer(input);
        let tokens = lexer.tokenize();
        assert_eq!(tokens.len(), 1);
        assert_eq!(tokens[0].kind, TokenKind::EMOJI_EMOTICONS);
        assert_eq!(tokens[0].text, "ğŸ˜€ğŸ˜ğŸ˜‚");
    }

    #[test]
    fn test_emoji_transport() {
        let input = "ğŸš€ğŸšğŸš‚";
        let mut lexer = TestUnicodeLexer(input);
        let tokens = lexer.tokenize();
        assert_eq!(tokens.len(), 1);
        assert_eq!(tokens[0].kind, TokenKind::EMOJI_TRANSPORT);
        assert_eq!(tokens[0].text, "ğŸš€ğŸšğŸš‚");
    }

    #[test]
    fn test_hiragana() {
        let input = "ã²ã‚‰ãŒãª";
        let mut lexer = TestUnicodeLexer(input);
        let tokens = lexer.tokenize();
        assert_eq!(tokens.len(), 1);
        assert_eq!(tokens[0].kind, TokenKind::HIRAGANA);
        assert_eq!(tokens[0].text, "ã²ã‚‰ãŒãª");
    }

    #[test]
    fn test_katakana() {
        let input = "ã‚«ã‚¿ã‚«ãƒŠ";
        let mut lexer = TestUnicodeLexer(input);
        let tokens = lexer.tokenize();
        assert_eq!(tokens.len(), 1);
        assert_eq!(tokens[0].kind, TokenKind::KATAKANA);
        assert_eq!(tokens[0].text, "ã‚«ã‚¿ã‚«ãƒŠ");
    }

    #[test]
    fn test_kanji() {
        let input = "æ¼¢å­—";
        let mut lexer = TestUnicodeLexer(input);
        let tokens = lexer.tokenize();
        assert_eq!(tokens.len(), 1);
        assert_eq!(tokens[0].kind, TokenKind::KANJI);
        assert_eq!(tokens[0].text, "æ¼¢å­—");
    }

    #[test]
    fn test_mixed_japanese() {
        let input = "ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—";
        let mut lexer = TestUnicodeLexer(input);
        let tokens = lexer.tokenize();
        assert_eq!(tokens.len(), 3);
        assert_eq!(tokens[0].kind, TokenKind::HIRAGANA);
        assert_eq!(tokens[0].text, "ã²ã‚‰ãŒãª");
        assert_eq!(tokens[1].kind, TokenKind::KATAKANA);
        assert_eq!(tokens[1].text, "ã‚«ã‚¿ã‚«ãƒŠ");
        assert_eq!(tokens[2].kind, TokenKind::KANJI);
        assert_eq!(tokens[2].text, "æ¼¢å­—");
    }

    #[test]
    fn test_cyrillic() {
        let input = "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚";
        let mut lexer = TestUnicodeLexer(input);
        let tokens = lexer.tokenize();
        assert_eq!(tokens.len(), 1);
        assert_eq!(tokens[0].kind, TokenKind::CYRILLIC);
        assert_eq!(tokens[0].text, "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚");
    }

    #[test]
    fn test_hangul() {
        let input = "í•œê¸€";
        let mut lexer = TestUnicodeLexer(input);
        let tokens = lexer.tokenize();
        assert_eq!(tokens.len(), 1);
        assert_eq!(tokens[0].kind, TokenKind::HANGUL);
        assert_eq!(tokens[0].text, "í•œê¸€");
    }

    #[test]
    fn test_arabic() {
        let input = "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©";
        let mut lexer = TestUnicodeLexer(input);
        let tokens = lexer.tokenize();
        assert_eq!(tokens.len(), 1);
        assert_eq!(tokens[0].kind, TokenKind::ARABIC);
        assert_eq!(tokens[0].text, "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©");
    }

    #[test]
    fn test_mixed_with_numbers() {
        let input = "ğŸ˜€123ã‚«ã‚¿ã‚«ãƒŠ456";
        let mut lexer = TestUnicodeLexer(input);
        let tokens = lexer.tokenize();
        assert_eq!(tokens.len(), 4);
        assert_eq!(tokens[0].kind, TokenKind::EMOJI_EMOTICONS);
        assert_eq!(tokens[0].text, "ğŸ˜€");
        assert_eq!(tokens[1].kind, TokenKind::NUMBER);
        assert_eq!(tokens[1].text, "123");
        assert_eq!(tokens[2].kind, TokenKind::KATAKANA);
        assert_eq!(tokens[2].text, "ã‚«ã‚¿ã‚«ãƒŠ");
        assert_eq!(tokens[3].kind, TokenKind::NUMBER);
        assert_eq!(tokens[3].text, "456");
    }

    #[test]
    fn test_emoji_with_whitespace() {
        let input = "ğŸ˜€ ğŸ˜ ğŸ˜‚";
        let mut lexer = TestUnicodeLexer(input);
        let tokens = lexer.tokenize();
        // Whitespace tokens are filtered out
        assert_eq!(tokens.len(), 5); // 3 emojis + 2 whitespaces (if not filtered)
    }

    #[test]
    fn test_hex_uppercase() {
        let input = "ABCXYZ";
        let mut lexer = TestUnicodeLexer(input);
        let tokens = lexer.tokenize();
        assert_eq!(tokens.len(), 1);
        assert_eq!(tokens[0].kind, TokenKind::UPPERCASE_HEX);
        assert_eq!(tokens[0].text, "ABCXYZ");
    }

    #[test]
    fn test_hex_lowercase() {
        let input = "abcxyz";
        let mut lexer = TestUnicodeLexer(input);
        let tokens = lexer.tokenize();
        assert_eq!(tokens.len(), 1);
        assert_eq!(tokens[0].kind, TokenKind::LOWERCASE_HEX);
        assert_eq!(tokens[0].text, "abcxyz");
    }

    #[test]
    fn test_hex_mixed() {
        let input = "ABC123abc";
        let mut lexer = TestUnicodeLexer(input);
        let tokens = lexer.tokenize();
        assert_eq!(tokens.len(), 3);
        assert_eq!(tokens[0].kind, TokenKind::UPPERCASE_HEX);
        assert_eq!(tokens[0].text, "ABC");
        assert_eq!(tokens[1].kind, TokenKind::NUMBER);
        assert_eq!(tokens[1].text, "123");
        assert_eq!(tokens[2].kind, TokenKind::LOWERCASE_HEX);
        assert_eq!(tokens[2].text, "abc");
    }
}
